page 255

6.1. Using the Wikipedia collection provided at the book website, create a sample
of stem clusters by the following process:
	1. Index the collection without stemming.
	
	2. Identify the first 1,000 words (in alphabetical order) in the index.
	
	3. Create stem classes by stemming these 1,000 words and recording which
	words become the same stem.
	
	4. Compute association measures (Dice’s coefficient) between all pairs of stems
	in each stem class. Compute co-occurrence at the document level.
	
	5. Create stem clusters by thresholding the association measure. All terms that
	are still connected to each other form the clusters.
	
Compare the stem clusters to the stem classes in terms of size and the quality (in
your opinion) of the groupings.

6.2. Create a simple spelling corrector based on the noisy channel model. Use a
single-word language model, and an error model where all errors with the same
edit distance have the same probability. Only consider edit distances of 1 or 2.
Implement your own edit distance calculator (example code can easily be found
on the Web).

6.3. Implement a simple pseudo-relevance feedback algorithm for the Galago
search engine. Provide examples of the query expansions that your algorithm does,
and summarize the problems and successes of your approach.

6.5. Describe the snippet generation algorithm in Galago. Would this algorithm
work well for pages with little text content? Describe in detail how you would
modify the algorithm to improve it.

6.7. Implement a simple algorithm that selects phrases from the top-ranked pages
as the basis for result clusters. Phrases should be considered as any two-word sequence.
Your algorithm should take into account phrase frequency in the results,
phrase frequency in the collection, and overlap in the clusters associated with the
phrases.


Page 319

7.2. Can you think of another measure of similarity that could be used in the vector
space model? Compare your measure with the cosine correlation using some
example documents and queries with made-up weights. Browse the IR literature
on the Web and see whether your measure has been studied (start with van Rijsbergen’s
book).

7.5. Implement a BM25 module for Galago. Show that it works and document
it.

7.6. Show the effect of changing parameter values in your BM25 implementation.

7.8. Using the Galago implementation of query likelihood, study the impact of
short queries and long queries on effectiveness. Do the parameter settings make a
difference?

7.13. Write an interface program that will take a user’s query as text and transform
it into an inference network query. Make sure you use proximity operators.
Compare the performance of the simple queries and the transformed queries.


MLN1: using the small wikipedia example, choose 10 words and create stem classes as per 
the algorithm on pp. 191-192

MLN2: using the small wikipedia example, choose 10 words and compute MIM, EMIM, chi square, 
dice association measures for full document & 5 word windows (cf. pp. 203-205)
